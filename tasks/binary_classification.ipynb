{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"clustering\")\n",
    "from utils import get_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def extract_and_process_large_json(input_file, train_file, test_file, target_label=\"Medicine\", limit=50000, test_size=0.2):\n",
    "    \"\"\"\n",
    "    从大 JSON 文件中提取目标数据，编码后分为训练集和测试集。\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): 输入 JSON 文件路径。\n",
    "        train_file (str): 输出训练集 JSON 文件路径。\n",
    "        test_file (str): 输出测试集 JSON 文件路径。\n",
    "        target_label (str): 目标类别标签。\n",
    "        limit (int): 每种类别的抽取数量。\n",
    "        test_size (float): 测试集比例。\n",
    "    \"\"\"\n",
    "    target_papers = []\n",
    "    non_target_papers = []\n",
    "    target_count = 0\n",
    "    non_target_count = 0\n",
    "\n",
    "    # 逐行读取大文件\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            try:\n",
    "                # 去掉多余逗号并解析 JSON\n",
    "                record = json.loads(line.strip(\",\\n\"))\n",
    "                scopus_label = record.get(\"Scopus_label\", \"\")\n",
    "                \n",
    "                # 分类数据\n",
    "                if scopus_label == target_label and target_count < limit:\n",
    "                    target_papers.append(record)\n",
    "                    target_count += 1\n",
    "                elif scopus_label != target_label and non_target_count < limit:\n",
    "                    non_target_papers.append(record)\n",
    "                    non_target_count += 1\n",
    "                \n",
    "                # 如果达到限制，停止读取\n",
    "                if target_count >= limit and non_target_count >= limit:\n",
    "                    break\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # 跳过解析失败的行\n",
    "    \n",
    "    print(f\"已提取 {len(target_papers)} 篇 {target_label} 和 {len(non_target_papers)} 篇非 {target_label} 的记录\")\n",
    "\n",
    "    # 合并数据\n",
    "    all_data = target_papers + non_target_papers\n",
    "\n",
    "    # 对 Scopus_label 进行编码\n",
    "    labels = [record[\"Scopus_label\"] for record in all_data]\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_labels = encoder.fit_transform(labels)\n",
    "    for i, record in enumerate(all_data):\n",
    "        record[\"Scopus_label\"] = int(encoded_labels[i])\n",
    "    \n",
    "    label_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "\n",
    "    # 分割数据为训练集和测试集\n",
    "    train_data, test_data = train_test_split(all_data, test_size=test_size, random_state=42)\n",
    "\n",
    "    # 保存到 JSON 文件\n",
    "    with open(train_file, 'w', encoding='utf-8') as trainfile:\n",
    "        json.dump(train_data, trainfile, indent=2)\n",
    "    with open(test_file, 'w', encoding='utf-8') as testfile:\n",
    "        json.dump(test_data, testfile, indent=2)\n",
    "    \n",
    "    print(f\"训练数据保存到 {train_file}，测试数据保存到 {test_file}\")\n",
    "    return label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dir:  /home/lyuzhuoqi/projects/clustering/data\n",
      "data_dir:  /home/lyuzhuoqi/projects/clustering/data\n",
      "data_dir:  /home/lyuzhuoqi/projects/clustering/data\n",
      "已提取 50000 篇 Medicine 和 50000 篇非 Medicine 的记录\n",
      "训练数据保存到 /home/lyuzhuoqi/projects/clustering/data/2010s/train.json，测试数据保存到 /home/lyuzhuoqi/projects/clustering/data/2010s/test.json\n"
     ]
    }
   ],
   "source": [
    "# 使用示例\n",
    "Scopus_label_map = extract_and_process_large_json(\n",
    "    input_file=os.path.join(get_data_dir(),\"2010s\",\"dataset.json\"),\n",
    "    train_file=os.path.join(get_data_dir(), \"2010s\",\"train.json\"),\n",
    "    test_file=os.path.join(get_data_dir(), \"2010s\",\"test.json\"),\n",
    "    target_label=\"Medicine\",\n",
    "    limit=50000,\n",
    "    test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(18)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scopus_label_map['Medicine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dir:  /home/lyuzhuoqi/projects/clustering/data\n",
      "data_dir:  /home/lyuzhuoqi/projects/clustering/data\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_data(train_file, test_file):\n",
    "    \"\"\"\n",
    "    加载训练集和测试集数据。\n",
    "    \n",
    "    Args:\n",
    "        train_file (str): 训练集文件路径。\n",
    "        test_file (str): 测试集文件路径。\n",
    "    \n",
    "    Returns:\n",
    "        list: 训练集数据。\n",
    "        list: 测试集数据。\n",
    "    \"\"\"\n",
    "    with open(train_file, 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "    return train_data, test_data\n",
    "\n",
    "# 加载数据\n",
    "train_file = os.path.join(get_data_dir(),\"2010s\",\"train.json\")\n",
    "test_file = os.path.join(get_data_dir(),\"2010s\",\"test.json\")\n",
    "train_data, test_data = load_data(train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0f7d8b18ff4756be79df220eb6c034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=3), Label(value='0 / 3'))), HBox(c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 自定义词映射\n",
    "word_mapping = {'mechanics': 'mechanic',\n",
    "                'mechanical': 'mechanic',\n",
    "                'electrical': 'electric',\n",
    "                'electronics': 'electric',\n",
    "                'financial': 'finance',\n",
    "                'political': 'politics',\n",
    "                'historical': 'history',\n",
    "                'computer': 'computing',\n",
    "                'intelligent': 'intelligence',\n",
    "                'agricultural': 'agriculture',\n",
    "                'educational': 'education',\n",
    "                'dental': 'dentistry',\n",
    "                'archaeological': 'archaeology',\n",
    "                'mathematical': 'mathematics',\n",
    "                'mathematica': 'mathematics',\n",
    "                'matematico': 'mathematics',\n",
    "                'mathematicae': 'mathematics',\n",
    "                'economic': 'economics',\n",
    "                'chemical': 'chemistry',\n",
    "                'geophysical': 'geophysics',\n",
    "                'botanical': 'botany',\n",
    "                'physical': 'physics',\n",
    "                'entomological': 'entomology', \n",
    "                'entomologist': 'entomology',\n",
    "                'biological': 'biology',\n",
    "                'geographical': 'geography',\n",
    "                'geological': 'geology',\n",
    "                'geographer': 'geography',\n",
    "                'cells': 'cell',\n",
    "                'policy': 'politics',\n",
    "}\n",
    "\n",
    "paper_df = pd.concat([pd.DataFrame(train_data), pd.DataFrame(test_data)])\n",
    "\n",
    "# 定义预处理函数，进行自定义词映射\n",
    "def preprocess_text(text):\n",
    "    words = text.lower().split()  # 将文本小写并按空格分词\n",
    "    mapped_words = [word_mapping[word] if word in word_mapping else word for word in words]\n",
    "    filtered_words = [word for word in mapped_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# 按 cluster_label 分组，将 OriginalVenue 文本合并为一个文档，并进行预处理\n",
    "cluster_docs = (\n",
    "    paper_df.groupby('cluster_label').parallel_apply(lambda x: preprocess_text(' '.join(x.abstract))).tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义停用词\n",
    "custom_stop_words = ['results', 'model', 'based', 'data', 'proposed', 'study', 'paper', 'results', \n",
    "                     'system', 'patients', 'using', 'method', 'health', 'two', 'article', 'la', 'que', 'en',\n",
    "                     'el', 'et', 'un', 'one', 'also', 'high', 'properties', 'methods', 'among', 'new',\n",
    "                     'sp', 'use', 'group', 'used', 'process', 'kg', 'abstract', 'different', 'time',\n",
    "                     'treatment', 'analysis', 'different', 'performance', 'le', 'los', 'se', 'les',\n",
    "                     'mm', 'may', 'de', 'del', 'des', 'nov', 'found', 'research', 'showed', 'las',\n",
    "                     'development', 'years', 'da', 'studies', 'first', 'findings', 'di', 'however', 'three',\n",
    "                     'associated', 'relationship', 'design', 'em', 'approach', 'risk', 'patient', 'care',\n",
    "                     '95', 'age', 'ci', 'compared', '10', 'background', 'conclusions', 'significant',\n",
    "                     'higher', 'vs', 'mortality', 'therapy', 'increased', 'significantly', '19', 'children',\n",
    "                     'find', 'show', 'problem', 'mathrm', 'included', 'mean', 'systems', '12', 'effects',\n",
    "                     'participants', 'related', 'cases', 'disease', 'levels', 'outcomes', 'non', 'total',\n",
    "                     'factors', 'conclusion', 'nm', 'ra', 'year', 'dr'\n",
    "                     ]\n",
    "\n",
    "# 计算 TF-IDF\n",
    "top_words_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english')+custom_stop_words)\n",
    "tfidf_matrix = top_words_vectorizer.fit_transform(cluster_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0): ['energy',\n",
       "  'temperature',\n",
       "  'heat',\n",
       "  'flow',\n",
       "  'water',\n",
       "  'surface',\n",
       "  'parameters',\n",
       "  'effect',\n",
       "  'experimental',\n",
       "  'conditions'],\n",
       " np.int64(1): ['algorithm',\n",
       "  'information',\n",
       "  'learning',\n",
       "  'image',\n",
       "  'network',\n",
       "  'models',\n",
       "  'images',\n",
       "  'features',\n",
       "  'propose',\n",
       "  'algorithms'],\n",
       " np.int64(2): ['social',\n",
       "  'management',\n",
       "  'firms',\n",
       "  'knowledge',\n",
       "  'value',\n",
       "  'purpose',\n",
       "  'work',\n",
       "  'business',\n",
       "  'information',\n",
       "  'literature'],\n",
       " np.int64(3): ['power',\n",
       "  'control',\n",
       "  'algorithm',\n",
       "  'network',\n",
       "  'voltage',\n",
       "  'energy',\n",
       "  'frequency',\n",
       "  'antenna',\n",
       "  'low',\n",
       "  'sensor'],\n",
       " np.int64(4): ['dogs',\n",
       "  'cows',\n",
       "  'animals',\n",
       "  'cell',\n",
       "  'cattle',\n",
       "  'days',\n",
       "  'clinical',\n",
       "  'horses',\n",
       "  'control',\n",
       "  'animal'],\n",
       " np.int64(5): ['plant',\n",
       "  'species',\n",
       "  'soil',\n",
       "  'acid',\n",
       "  'plants',\n",
       "  'growth',\n",
       "  'content',\n",
       "  'protein',\n",
       "  'yield',\n",
       "  'production'],\n",
       " np.int64(6): ['archaeology',\n",
       "  'human',\n",
       "  'holocene',\n",
       "  'microwear',\n",
       "  'pleistocene',\n",
       "  'heritage',\n",
       "  'sites',\n",
       "  'cultural',\n",
       "  'history',\n",
       "  'century'],\n",
       " np.int64(7): ['dentistry',\n",
       "  'bone',\n",
       "  'oral',\n",
       "  'implant',\n",
       "  'clinical',\n",
       "  'surgery',\n",
       "  'groups',\n",
       "  'teeth',\n",
       "  'surgical',\n",
       "  'mandibular'],\n",
       " np.int64(8): ['clinical',\n",
       "  'brain',\n",
       "  'stroke',\n",
       "  'cell',\n",
       "  'cognitive',\n",
       "  'pain',\n",
       "  'symptoms',\n",
       "  'surgery',\n",
       "  'eyes',\n",
       "  'dr'],\n",
       " np.int64(9): ['temperature',\n",
       "  'energy',\n",
       "  'surface',\n",
       "  'structure',\n",
       "  'material',\n",
       "  'parameters',\n",
       "  'obtained',\n",
       "  'materials',\n",
       "  'effect',\n",
       "  'composites'],\n",
       " np.int64(10): ['pain',\n",
       "  'knee',\n",
       "  'bone',\n",
       "  'hip',\n",
       "  'surgery',\n",
       "  'clinical',\n",
       "  'postoperative',\n",
       "  'fractures',\n",
       "  'surgical',\n",
       "  'fracture'],\n",
       " np.int64(11): ['students',\n",
       "  'education',\n",
       "  'learning',\n",
       "  'teachers',\n",
       "  'school',\n",
       "  'teaching',\n",
       "  'student',\n",
       "  'academic',\n",
       "  'teacher',\n",
       "  'language'],\n",
       " np.int64(12): ['social',\n",
       "  'mental',\n",
       "  'self',\n",
       "  'women',\n",
       "  'symptoms',\n",
       "  'depression',\n",
       "  'clinical',\n",
       "  'reported',\n",
       "  'support',\n",
       "  'intervention'],\n",
       " np.int64(13): ['species',\n",
       "  'habitat',\n",
       "  'populations',\n",
       "  'fish',\n",
       "  'diversity',\n",
       "  'population',\n",
       "  'habitats',\n",
       "  'plant',\n",
       "  'genus',\n",
       "  'forest'],\n",
       " np.int64(14): ['network',\n",
       "  'security',\n",
       "  'algorithm',\n",
       "  'iot',\n",
       "  'software',\n",
       "  'cloud',\n",
       "  'computing',\n",
       "  'information',\n",
       "  'propose',\n",
       "  'applications'],\n",
       " np.int64(15): ['clinical',\n",
       "  'cell',\n",
       "  'blood',\n",
       "  'hospital',\n",
       "  'medical',\n",
       "  'year',\n",
       "  'groups',\n",
       "  'cardiac',\n",
       "  'months',\n",
       "  'ra'],\n",
       " np.int64(16): ['ensuremath',\n",
       "  'quantum',\n",
       "  'equations',\n",
       "  'field',\n",
       "  'order',\n",
       "  'mathbb',\n",
       "  'solutions',\n",
       "  'equation',\n",
       "  'function',\n",
       "  'state'],\n",
       " np.int64(17): ['market',\n",
       "  'firms',\n",
       "  'economics',\n",
       "  'countries',\n",
       "  'politics',\n",
       "  'level',\n",
       "  'finance',\n",
       "  'price',\n",
       "  'effect',\n",
       "  'trade'],\n",
       " np.int64(18): ['water',\n",
       "  'soil',\n",
       "  'area',\n",
       "  'surface',\n",
       "  'temperature',\n",
       "  'climate',\n",
       "  'concentrations',\n",
       "  'well',\n",
       "  'low',\n",
       "  'land'],\n",
       " np.int64(19): ['politics',\n",
       "  'social',\n",
       "  'state',\n",
       "  'public',\n",
       "  'international',\n",
       "  'countries',\n",
       "  'states',\n",
       "  'economics',\n",
       "  'history',\n",
       "  'law'],\n",
       " np.int64(20): ['cancer',\n",
       "  'cell',\n",
       "  'tumor',\n",
       "  'clinical',\n",
       "  'surgery',\n",
       "  'survival',\n",
       "  'liver',\n",
       "  'breast',\n",
       "  'expression',\n",
       "  'performed'],\n",
       " np.int64(21): ['hiv',\n",
       "  'women',\n",
       "  'infection',\n",
       "  'clinical',\n",
       "  'prevalence',\n",
       "  'control',\n",
       "  'positive',\n",
       "  'covid',\n",
       "  'infections',\n",
       "  'hospital'],\n",
       " np.int64(22): ['cell',\n",
       "  'cancer',\n",
       "  'expression',\n",
       "  'protein',\n",
       "  'breast',\n",
       "  'cells',\n",
       "  'tumor',\n",
       "  'mice',\n",
       "  'induced',\n",
       "  'genes'],\n",
       " np.int64(23): ['temperature',\n",
       "  'surface',\n",
       "  'structure',\n",
       "  'energy',\n",
       "  'electron',\n",
       "  'phase',\n",
       "  'low',\n",
       "  'cell',\n",
       "  'metal',\n",
       "  'effect'],\n",
       " np.int64(24): ['social',\n",
       "  'information',\n",
       "  'language',\n",
       "  'work',\n",
       "  'learning',\n",
       "  'cognitive',\n",
       "  'present',\n",
       "  'theory',\n",
       "  'well',\n",
       "  'como'],\n",
       " np.int64(25): ['history',\n",
       "  'politics',\n",
       "  'century',\n",
       "  'literary',\n",
       "  'war',\n",
       "  'du',\n",
       "  'social',\n",
       "  'cultural',\n",
       "  'una',\n",
       "  'como']}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = 10  # 每个聚类提取前 n 个关键词\n",
    "# 获取每个聚类中最重要的关键词\n",
    "top_words = {}\n",
    "feature_names = top_words_vectorizer.get_feature_names_out()\n",
    "\n",
    "# 确保 cluster_label 的顺序与 cluster_docs 一致\n",
    "cluster_labels = sorted(paper_df['cluster_label'].unique())\n",
    "\n",
    "for j, cluster_label in enumerate(cluster_labels):\n",
    "    tfidf_scores = tfidf_matrix[j].toarray().flatten()\n",
    "    top_indices = tfidf_scores.argsort()[-top_n:][::-1]  # 获取前 n 个关键词的索引\n",
    "    top_words[cluster_label] = [feature_names[idx] for idx in top_indices]\n",
    "\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def preprocess_data(data, label_field, target_value=None):\n",
    "    \"\"\"\n",
    "    对数据进行预处理，提取文本特征和标签。\n",
    "    \n",
    "    Args:\n",
    "        data (list): 数据列表。\n",
    "        label_field (str): 目标标签字段名（如 Scopus_label 或 cluster_label）。\n",
    "        target_value (str or int): 目标分类的值（如 Medicine 或 1）。\n",
    "    \n",
    "    Returns:\n",
    "        list: 文本数据列表。\n",
    "        list: 标签数据列表。\n",
    "    \"\"\"\n",
    "    texts = [item[\"abstract\"] for item in data]\n",
    "    if target_value is not None:\n",
    "        labels = [1 if item[label_field] == target_value else 0 for item in data]\n",
    "    else:\n",
    "        labels = [item[label_field] for item in data]\n",
    "    return texts, labels\n",
    "\n",
    "def train_and_evaluate_with_tfidf(vectorizer, train_texts, train_labels, test_texts, test_labels):\n",
    "    \"\"\"\n",
    "    使用预训练的 TF-IDF 进行向量化并训练 Random Forest 模型。\n",
    "    \n",
    "    Args:\n",
    "        vectorizer (TfidfVectorizer): 已拟合的 TF-IDF 向量化器。\n",
    "        train_texts (list): 训练文本数据。\n",
    "        train_labels (list): 训练标签数据。\n",
    "        test_texts (list): 测试文本数据。\n",
    "        test_labels (list): 测试标签数据。\n",
    "    \"\"\"\n",
    "    # 使用已拟合的 TF-IDF 进行向量化\n",
    "    X_train = vectorizer.transform(train_texts)\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "    # 随机森林分类器\n",
    "    classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    classifier.fit(X_train, train_labels)\n",
    "\n",
    "    # 预测与评估\n",
    "    predictions = classifier.predict(X_test)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(test_labels, predictions))\n",
    "    print(\"Test accuracy:\", accuracy_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline task: Binary classification based on Scopus_label (Medicine vs Others)\n",
      "Fitting TF-IDF vectorizer...\n",
      "Training and evaluating Random Forest classifier...\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88      9965\n",
      "           1       0.87      0.91      0.89     10035\n",
      "\n",
      "    accuracy                           0.89     20000\n",
      "   macro avg       0.89      0.89      0.89     20000\n",
      "weighted avg       0.89      0.89      0.89     20000\n",
      "\n",
      "Test accuracy: 0.88795\n"
     ]
    }
   ],
   "source": [
    "# Baseline task: 使用 Scopus_label 进行分类 (Medicine vs Others)\n",
    "print(\"\\nBaseline task: Binary classification based on Scopus_label (Medicine vs Others)\")\n",
    "train_texts, train_labels = preprocess_data(train_data, label_field=\"Scopus_label\", target_value=Scopus_label_map['Medicine'])\n",
    "test_texts, test_labels = preprocess_data(test_data, label_field=\"Scopus_label\", target_value=Scopus_label_map['Medicine'])\n",
    "\n",
    "# 拟合 TF-IDF（仅在训练数据上）\n",
    "print(\"Fitting TF-IDF vectorizer...\")\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words=stopwords.words('english'))\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "print(\"Training and evaluating Random Forest classifier...\")\n",
    "train_and_evaluate_with_tfidf(vectorizer, train_texts, train_labels, test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task 2: Binary classification based on cluster_label (current category vs others)\n",
      "All cluster labels and corresponding paper number: [(15, 17818), (22, 11816), (20, 11564), (23, 11481), (12, 6413), (8, 5264), (0, 5226), (21, 3708), (18, 3239), (10, 2911), (5, 2471), (3, 2354), (9, 2035), (16, 1890), (13, 1750), (19, 1533), (1, 1453), (2, 1267), (24, 1179), (11, 956), (7, 878), (4, 857), (17, 850), (25, 517), (14, 380), (6, 190)]\n",
      "\n",
      "Current target binary classification category: cluster_label = 15\n",
      "Top words: ['clinical', 'cell', 'blood', 'hospital', 'levels']\n",
      "Number of papers: 17818 (17.818 %)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92     16408\n",
      "           1       0.81      0.29      0.43      3592\n",
      "\n",
      "    accuracy                           0.86     20000\n",
      "   macro avg       0.84      0.64      0.68     20000\n",
      "weighted avg       0.86      0.86      0.83     20000\n",
      "\n",
      "Test accuracy: 0.86125\n",
      "\n",
      "Current target binary classification category: cluster_label = 22\n",
      "Top words: ['cell', 'cancer', 'expression', 'protein', 'breast']\n",
      "Number of papers: 11816 (11.816 %)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95     17622\n",
      "           1       0.76      0.29      0.42      2378\n",
      "\n",
      "    accuracy                           0.91     20000\n",
      "   macro avg       0.84      0.64      0.69     20000\n",
      "weighted avg       0.89      0.91      0.89     20000\n",
      "\n",
      "Test accuracy: 0.9053\n",
      "\n",
      "Current target binary classification category: cluster_label = 20\n",
      "Top words: ['cancer', 'cell', 'tumor', 'clinical', 'surgery']\n",
      "Number of papers: 11564 (11.564 %)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     17699\n",
      "           1       0.78      0.30      0.43      2301\n",
      "\n",
      "    accuracy                           0.91     20000\n",
      "   macro avg       0.85      0.64      0.69     20000\n",
      "weighted avg       0.90      0.91      0.89     20000\n",
      "\n",
      "Test accuracy: 0.90945\n",
      "\n",
      "Current target binary classification category: cluster_label = 23\n",
      "Top words: ['temperature', 'surface', 'structure', 'energy', 'electron']\n",
      "Number of papers: 11481 (11.481 %)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96     17725\n",
      "           1       0.81      0.58      0.68      2275\n",
      "\n",
      "    accuracy                           0.94     20000\n",
      "   macro avg       0.88      0.78      0.82     20000\n",
      "weighted avg       0.93      0.94      0.93     20000\n",
      "\n",
      "Test accuracy: 0.93665\n",
      "\n",
      "Current target binary classification category: cluster_label = 12\n",
      "Top words: ['social', 'mental', 'self', 'women', 'symptoms']\n",
      "Number of papers: 6413 (6.413 %)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97     18672\n",
      "           1       0.72      0.04      0.08      1328\n",
      "\n",
      "    accuracy                           0.94     20000\n",
      "   macro avg       0.83      0.52      0.52     20000\n",
      "weighted avg       0.92      0.94      0.91     20000\n",
      "\n",
      "Test accuracy: 0.93525\n",
      "\n",
      "Current target binary classification category: cluster_label = 8\n",
      "Top words: ['clinical', 'brain', 'stroke', 'cell', 'cognitive']\n",
      "Number of papers: 5264 (5.264 %)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[176], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m test_texts, test_labels \u001b[38;5;241m=\u001b[39m preprocess_data(test_data, label_field\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster_label\u001b[39m\u001b[38;5;124m\"\u001b[39m, target_value\u001b[38;5;241m=\u001b[39mcluster)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 复用已拟合的 TF-IDF\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain_and_evaluate_with_tfidf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[172], line 37\u001b[0m, in \u001b[0;36mtrain_and_evaluate_with_tfidf\u001b[0;34m(vectorizer, train_texts, train_labels, test_texts, test_labels)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m使用预训练的 TF-IDF 进行向量化并训练 Random Forest 模型。\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    test_labels (list): 测试标签数据。\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 使用已拟合的 TF-IDF 进行向量化\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m X_test \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(test_texts)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 随机森林分类器\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/p2v/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2115\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2098\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[1;32m   2099\u001b[0m \n\u001b[1;32m   2100\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2113\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2115\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/p2v/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1417\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1417\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1419\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/p2v/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/miniconda3/envs/p2v/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 任务 2: 使用 cluster_label 进行分类 (每个类 vs 非当前类)\n",
    "print(\"\\nTask 2: Binary classification based on cluster_label (current category vs others)\")\n",
    "cluster_label_count = Counter([item[\"cluster_label\"] for item in train_data + test_data])\n",
    "print(\"All cluster labels and corresponding paper number:\", cluster_label_count.most_common())\n",
    "for tup in cluster_label_count.most_common():\n",
    "    cluster = tup[0]\n",
    "    paper_number = tup[1]\n",
    "    print(f\"\\nCurrent target binary classification category: cluster_label = {cluster}\")\n",
    "    print(\"Top words:\", top_words[cluster])\n",
    "    print(f\"Number of papers: {paper_number} ({paper_number/(cluster_label_count.total())*100} %)\")\n",
    "    train_texts, train_labels = preprocess_data(train_data, label_field=\"cluster_label\", target_value=cluster)\n",
    "    test_texts, test_labels = preprocess_data(test_data, label_field=\"cluster_label\", target_value=cluster)\n",
    "    # 复用已拟合的 TF-IDF\n",
    "    train_and_evaluate_with_tfidf(vectorizer, train_texts, train_labels, test_texts, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
